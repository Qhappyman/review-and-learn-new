做前端离不开浏览器，浏览器离不开搜索引擎，最近看了一下vue的服务器端渲染，ssr提高了搜索引擎优化，便想着为什么会提高

+ SSR：

SSR是**Server-Side Rendering**, 即服务器端渲染的缩写，一般的页面是客户端将请求发送到服务器，然后服务器返回网站代码，进而浏览器解析js，css等文件生成DOM。SSR就是另一种方式，**依靠服务器****生成html**页面，然后发送到浏览器，最后再配合ajax等脚本代码进行交互，可以明显**优化首屏加载速度**。来看俩张图：

![cF3LN.png](https://ww1.yunjiexi.club/2020/03/09/cF3LN.png)

![cFrvF.png](https://ww1.yunjiexi.club/2020/03/09/cFrvF.png)

从图中可以明显看出：

1. ssr是服务器端请求数据，只要我们有好的服务器，带宽，而且是内网请求，数据的响应明显要快许多。
2. html页面服务器先生成，返回给客户端，至少我们给了客户一个完整可视的页面，进而的js文件慢慢加载也不影响用户体验。而客户端渲染，需要浏览器一步步去渲染css，html，当网络拥堵，或者文件较大，数据较多时，很容易造成白屏，体验极差。

**ssr优缺点**：

优点：

1. 有利于首屏渲染，不需要等待js文件下载完毕再加载页面
2. 有利于seo，因为html在服务器端就渲染好了，爬虫可以更舒服爬取到关键数据

缺点：

1. ssr需要性能好的服务器，由于都在服务器进行渲染，给服务器的压力很大
2. 受开发环境的影响，只会执行Component Mount之前的生命周期钩子，对第三方库有很大的限制
3. 可能由于特殊原因导致浏览器端和服务器端渲染的结果不一致

+ 爬虫和seo

    + 爬虫是什么？

        爬虫也被称为web机器人，spider等，是可以自动在网页之间进行事务处理的程序，服务器每天将大量的爬虫程序发散到浏览器，像人一样，他们也可以进入页面，查看内容，搜索关键信息，并对web站点进行遍历，层层深入，并把遇到的文档全都拉回到服务器上面，然后将数据进行分类处理，存放在专门的文件数据库中。

    + seo优化

        为了让一个网站能准确，快速地被爬虫识别，seo必不可少，seo是利用搜索引擎的排名规则，分析怎样抓取页面，确定关键词，优化搜索效率的一种技术，好的seo可以提高网站在搜索引擎中的排名。

        对于前端开发人员，注意seo也是一项重要的技能， 稍微总结了一下seo优化：

        1. 合理利用关键字，title，description，keywords，meta元信息要处理妥当，尽量简洁可懂，长度也不宜过长，能少则少
        2. HTML代码符合W3C标准，善于利用语义化标签，爬虫爬取的是html代码，如果html代码写的语义化，爬虫也会很容易明白所要表达的意思
        3. 少使用iframe框架，因为搜索引擎不会分析iframe里面的内容
        4. 图片加入alt，title，alt是css中代替图片无法显示的文字
        5. 关键信息不要放在隐藏，比如display：none， 爬虫不会检索隐藏的文字
        6. js尽可能与html分离，因为爬虫不会检索js
        7. 代码要简洁，文件要小，使网页相应快，不要搞很多花里胡哨的东西，表达关键信息就好，这也是对爬虫的友好

+ 搜索引擎工作过程

    搜索引擎的整个工作过程视为三个部分：

    1. 蜘蛛在互联网上爬行和抓取网页信息，并存入原始网页数据库
    2. 对原始网页数据库中的信息进行提取和组织，并建立索引库
    3. 根据用户输入的关键词，快速找到相关文档，并对找到的结果进行排序，并将查询结果返回给用户

    具体过程：

    1. 网页抓取
        Spider每遇到一个新文档，都要搜索其页面的链接网页。引擎蜘蛛先向页面提出访问请求，服务器接受其访问请求并返回HTML代码后，把获取的HTML代码存入原始页面数据库。搜索引擎使用多个蜘蛛分布爬行以提高爬行速度。搜索引擎的服务器遍布世界各地，每一台服务器都会派出多只蜘蛛同时去抓取网页。在抓取网页时，搜索引擎会**建立两张不同的表**，一张表记录已经访问过的网站，一张表记录没有访问过的网站。当蜘蛛抓取某个外部链接页面URL的时候，需把该网站的URL下载回来分析，当蜘蛛全部分析完这个URL后，将这个URL存入相应的表中，这时当另外的蜘蛛从其他的网站或页面又发现了这个URL时，它会对比看看已访问列表有没有，如果有，蜘蛛会自动丢弃该URL，不再访问。
    2. 预处理，建立索引
        为了便于用户大量网页数据库中快速便捷地找到搜索结果，搜索引擎必须将spider抓取的原始web页面做**预处理**。网页预处理最主要过程是为网页**建立全文索引**，之后开始分析网页，最后建立倒排文件（也称反向索引）。Web页面分析有以下步骤：判断网页类型，衡量其重要程度，丰富程度，对超链接进行分析，分词，把重复网页去掉。经过搜索引擎分析处理后，web网页已经不再是原始的网页页面，而是浓缩成能反映页面主题内容的、以词为单位的文档。数据索引中结构最复杂的是建立索引库，索引又分为文档索引和关键词索引。每个网页唯一的docID号是有文档索引分配的，每个wordID出现的次数、位置、大小格式都可以根据docID号在网页中检索出来。最终形成wordID的数据列表。倒排索引形成过程是这样的：搜索引擎用分词系统将文档自动切分成单词序列-对每个单词赋予唯一的单词编号-记录包含这个单词的文档。倒排索引是最简单的，实用的倒排索引还需记载更多的信息。在单词对应的倒排列表除了记录文档编号之外，单词频率信息也被记录进去，便于以后计算查询和文档的相似度。
    3. 查询服务
        在搜索引擎界面输入关键词，点击“搜索”按钮之后，搜索引擎程序开始对搜索词进行以下处理：分词处理、根据情况对整合搜索是否需要启动进行判断、找出错别字和拼写中出现的错误、把停止词去掉。接着搜索引擎程序便把包含搜索词的相关网页从索引数据库中找出，而且对网页进行排序，最后按照一定格式返回到“搜索”页面。查询服务最核心的部分是搜索结果排序，其决定了搜索引擎的量好坏及用户满意度。实际搜索结果排序的因子很多，但最主要的因素之一是网页内容的相关度。影响相关性的主要因素包括如下五个方面。
        （1）关键词常用程度。经过分词后的多个关键词，对整个搜索字符串的意义贡献并不相同。越常用的词对搜索词的意义贡献越小，越不常用的词对搜索词的意义贡献越大。常用词发展到一定极限就是停止词，对页面不产生任何影响。所以搜索引擎用的词加权系数高，常用词加权系数低，排名算法更多关注的是不常用的词。
        （2）词频及密度。通常情况下，搜索词的密度和其在页面中出现的次数成正相关，次数越多，说明密度越大，页面与搜索词关系越密切。
        （3）关键词位置及形式。关键词出现在比较重要的位置，如标题标签、黑体、H1等，说明页面与关键词越相关。在索引库的建立中提到的，页面关键词出现的格式和位置都被记录在索引库中。
        （4）关键词距离。关键词被切分之后，如果匹配的出现，说明其与搜索词相关程度越大，当“搜索引擎”在页面上连续完整的出现或者“搜索”和“引擎”出现的时候距离比较近，都被认为其与搜索词相关。
        （5）链接分析及页面权重。页面之间的链接和权重关系也影响关键词的相关性，其中最重要的是锚文字。页面有越多以搜索词为锚文字的导入链接，说明页面的相关性越强。链接分析还包括了链接源页面本身的主题、锚文字周围的文字等

    

    

    

    

    

    